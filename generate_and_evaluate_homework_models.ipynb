{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and evaluate homework models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, mean_squared_error as mse\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PU_Learning import *\n",
    "\n",
    "def ignore_warnings():\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "ignore_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "data = pd.read_csv('clean_data/players_17_clean_train.csv')\n",
    "train_x = data.iloc[:, :-1].values\n",
    "train_y = data.iloc[:, -1].values\n",
    "s = data.iloc[:, -1].values\n",
    "c = Counter(s)[1]/Counter(train_y)[1]\n",
    "\n",
    "# import test data \n",
    "data = pd.read_csv('clean_data/players_17_clean_test.csv')\n",
    "test_x = data.iloc[:, :-1].values\n",
    "test_y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Golden Standard Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden Standard Classifier: F1 score: 0.6966759002770083\n"
     ]
    }
   ],
   "source": [
    "# consider the dataset as fully labeled and use this as the best case in the comparison.\n",
    "golden_clf = svm.SVC(kernel='rbf', probability=True, random_state=331).fit(np.copy(train_x),np.copy(train_y))\n",
    "name = \"Golden Standard Classifier:\"\n",
    "\n",
    "best_pred_y = golden_clf.predict(np.copy(test_x))\n",
    "best_prob_y = golden_clf.predict_proba(np.copy(test_x))[:,1]\n",
    "\n",
    "print(name,\"F1 score:\", f1_score(test_y, best_pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Traditional Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Traditional Classifier F1 score: 0.6966759002770083\n",
      "Non-Traditional Classifier MSE score: 0.0\n"
     ]
    }
   ],
   "source": [
    "non_trad_clf = svm.SVC(kernel='rbf', probability=True, random_state=331).fit(np.copy(train_x), np.copy(s))\n",
    "name = \"Non-Traditional Classifier\"\n",
    "\n",
    "pred_y = non_trad_clf.predict(np.copy(test_x))\n",
    "pred_prob_y = non_trad_clf.predict_proba(np.copy(test_x))[:,1]\n",
    "\n",
    "print(name,\"F1 score:\", f1_score(test_y, pred_y))\n",
    "print(name,\"MSE score:\", mse(best_prob_y, pred_prob_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spy Expectation Maximization S-EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations first step: 229\n",
      "Number of iterations second step: 212\n",
      "SEM F1 score: 0.3798687089715536\n",
      "SEM MSE score: 0.7290650355068272\n"
     ]
    }
   ],
   "source": [
    "pu_classifier = SEM(tol=1.0e-10, max_iter=1000, spy_prop=0.1, l=0.15, classifier=LogisticRegression(), seed=331)\n",
    "name = \"SEM\"\n",
    "\n",
    "pu_classifier.fit(np.copy(train_x), np.copy(s))\n",
    "\n",
    "pred_y = pu_classifier.predict(np.copy(test_x))\n",
    "pred_prob_y = pu_classifier.predict_proba(np.copy(test_x))[:,1]\n",
    "\n",
    "print(name,\"F1 score:\", f1_score(test_y, pred_y))\n",
    "print(name,\"MSE score:\", mse(best_prob_y, pred_prob_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Logistic Regression MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR F1 score: 0.3798687089715536\n",
      "MLR MSE score: 0.7303110999703546\n"
     ]
    }
   ],
   "source": [
    "pu_classifier = ModifiedLogisticRegression(max_iter=1000, l_rate=0.001, seed=331)\n",
    "name = \"MLR\"\n",
    "\n",
    "pu_classifier.parameters_update(np.copy(train_x), np.copy(s))\n",
    "pu_classifier.fit(np.copy(train_x), np.copy(s))\n",
    "pu_classifier.estimate_c()\n",
    "\n",
    "pred_y = pu_classifier.predict(np.copy(test_x))\n",
    "pred_prob_y = pu_classifier.predict_proba(np.copy(test_x))\n",
    "\n",
    "print(name,\"F1 score:\", f1_score(test_y, pred_y))\n",
    "print(name,\"MSE score:\", mse(best_prob_y, pred_prob_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
